__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
import streamlit as st

load_dotenv()

# ì œëª©
st.title("Chat-YouJeong ğŸ“®")
st.write('---')

# Documnet Loaders
loader = TextLoader("./seed.md")
pages = loader.load_and_split()  # í˜ì´ì§€ ë³„ë¡œ ìª¼ê°¬

# Document Transformers
text_splitter: list = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 300,  # Nê¸€ì ë‹¨ìœ„ë¡œ ìë¥´ê² ë‹¤.
    chunk_overlap  = 20,  # ê²¹ì¹˜ëŠ” ê¸€ì
    length_function = len,
    is_separator_regex = False,
)
texts: list = text_splitter.split_documents(pages)
# print(texts[0])

# Text embedding models(ì„ë² ë”©, OpenAI API, ìœ ë£Œ)
# ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ëŠ¥ë ¥ = ì„ë² ë”©
# ì„ë² ë”© ëª¨ë¸ ì“°ë ¤ë©´, tiktoken í•„ìš” 
embeddings_model = OpenAIEmbeddings()  # .envì— ìˆëŠ” KEYë¥¼ ìë™ìœ¼ë¡œ ì½ì–´ì˜´

# load it into Chroma (now, in-memory)
vectordb = Chroma.from_documents(texts, embeddings_model)

# Question
st.header("YouJeong GPTì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!!")
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

if st.button("ì§ˆë¬¸í•˜ê¸°"):
    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)
    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())
    result = qa_chain({"query": question})
    st.write(result['result'])
